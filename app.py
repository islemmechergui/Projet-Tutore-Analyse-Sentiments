import streamlit as st
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
from wordcloud import WordCloud
from typing import List, Dict, Any
from transformers import pipeline
import io

from classification import run_classification

st.set_page_config(
    page_title="Analyse de Sentiments Amazon",
    page_icon="üß†",
    layout="wide"
)

st.markdown("""
<style>
.main {background-color: #f7f9fc;}
h1 {color:#1f4e79;}
h2 {color:#16365d;}
h3 {color:#2e75b6;}
</style>
""", unsafe_allow_html=True)

# ================================
# Fonctions Transformers avec cache Streamlit
# ================================

DEFAULT_HF_MODEL = "cmarkea/distilcamembert-base-sentiment"

@st.cache_resource(show_spinner="üîÑ Chargement du mod√®le Transformers...")
def get_sentiment_pipeline(model_name: str = DEFAULT_HF_MODEL):
    """Charge et cache le pipeline HF pour analyse de sentiments"""
    try:
        return pipeline(
            task="sentiment-analysis",
            model=model_name,
            tokenizer=model_name,
            device=-1,
            max_length=512,
            truncation=True
        )
    except Exception as e:
        st.error(f"‚ùå Erreur chargement mod√®le '{model_name}': {str(e)}")
        return None

def normalize_label(raw_label: str, model_name: str) -> int:
    """Mappe les labels HF vers {-1, 0, 1}"""
    if raw_label is None:
        return 0
    
    lbl = str(raw_label).strip().lower()
    
    if any(k in lbl for k in ["neg", "-1", "negative"]):
        return -1
    if any(k in lbl for k in ["neu", "neutral", "0"]):
        return 0
    if any(k in lbl for k in ["pos", "+1", "positive"]):
        return 1
    
    for d in ["1", "2", "3", "4", "5"]:
        if d in lbl and "star" in lbl:
            val = int(d)
            if val <= 2:
                return -1
            elif val == 3:
                return 0
            else:
                return 1
    
    return 0

def hf_predict_text(text: str, model_name: str = DEFAULT_HF_MODEL) -> Dict[str, Any]:
    """Pr√©dit le sentiment d'un texte unique"""
    try:
        nlp = get_sentiment_pipeline(model_name)
        if nlp is None:
            return {"error": "Pipeline indisponible"}
        
        text_truncated = text[:2000]
        out = nlp(text_truncated, truncation=True, max_length=512)[0]
        
        return {
            "label": out.get("label"),
            "score": float(out.get("score", 0.0)),
            "mapped": normalize_label(out.get("label"), model_name),
            "model": model_name
        }
    except Exception as e:
        return {"error": f"Erreur pr√©diction: {str(e)}"}

def hf_predict_batch(texts: List[str], model_name: str = DEFAULT_HF_MODEL) -> List[Dict[str, Any]]:
    """Pr√©dit le sentiment de plusieurs textes"""
    try:
        nlp = get_sentiment_pipeline(model_name)
        if nlp is None:
            return []
        
        texts_truncated = [t[:2000] for t in texts]
        outputs = nlp(texts_truncated, truncation=True, max_length=512)
        
        results = []
        for t, r in zip(texts, outputs):
            results.append({
                "text": t,
                "label": r.get("label"),
                "score": float(r.get("score", 0.0)),
                "mapped": normalize_label(r.get("label"), model_name),
                "model": model_name
            })
        return results
    except Exception as e:
        st.error(f"‚ùå Erreur batch: {str(e)}")
        return []

# ================================
# Navigation et chargement donn√©es
# ================================

st.sidebar.header("üìå Navigation")
page = st.sidebar.radio(
    "Choisissez la page",
    [
        "Introduction",
        "Statistiques & Graphes",
        "Classification des Sentiments",
        "Analyse via Transformers",
        "Dataset Nettoy√©"
    ]
)

st.sidebar.header("‚öôÔ∏è Param√®tres")
path_cleaned = st.sidebar.text_input(
    "Chemin du dataset nettoy√©",
    "data/amazon_reviews_cleaned.csv"
)

@st.cache_data
def load_data(path):
    return pd.read_csv(path)

try:
    df = load_data(path_cleaned)
except Exception as e:
    st.error(f"Erreur de chargement : {e}")
    st.stop()

review_text_col = next(
    (c for c in df.columns if "review" in c.lower() and "text" in c.lower()),
    None
)

review_title_col = next(
    (c for c in df.columns if "title" in c.lower() or "summary" in c.lower()),
    None
)

country_col = next(
    (c for c in df.columns if "country" in c.lower()),
    None
)

if "label" not in df.columns and "numeric_rating" in df.columns:
    def rating_to_label(r):
        if r <= 2:
            return -1
        elif r == 3:
            return 0
        else:
            return 1
    df["label"] = df["numeric_rating"].apply(rating_to_label)

if page == "Introduction":
    st.title("üß† Analyse de Sentiments sur les Avis Amazon")

    st.markdown("""
    **Objectif du projet :**  
    Analyser les avis clients Amazon afin de :
    - Nettoyer et structurer des donn√©es textuelles
    - Analyser les sentiments (positif / neutre / n√©gatif)
    - Comparer des mod√®les de classification supervis√©e
    - Visualiser les r√©sultats de mani√®re interactive

    **M√©thodologie :**
    - Pr√©traitement NLP (tokenisation, stopwords, lemmatisation)
    - Vectorisation TF-IDF
    - Mod√®les : Logistic Regression, Naive Bayes
    - Visualisation avec Streamlit
    """)

    st.subheader("Aper√ßu du dataset")
    st.write(f"Nombre total de reviews : **{len(df)}**")
    st.dataframe(df.head(30))

elif page == "Statistiques & Graphes":
    st.title("üìä Statistiques & Visualisations")

    col1, col2, col3, col4 = st.columns(4)
    col1.metric("Total reviews", len(df))
    col2.metric("Positives", (df["label"] == 1).sum())
    col3.metric("Neutres", (df["label"] == 0).sum())
    col4.metric("N√©gatives", (df["label"] == -1).sum())

    st.subheader("R√©partition des sentiments")
    fig, ax = plt.subplots()
    sns.countplot(x="label", data=df, ax=ax)
    ax.set_xlabel("Sentiment (-1, 0, 1)")
    ax.set_ylabel("Nombre")
    st.pyplot(fig)

    if "numeric_rating" in df.columns:
        st.subheader("Distribution des ratings")
        fig, ax = plt.subplots()
        sns.histplot(df["numeric_rating"], bins=5, ax=ax)
        st.pyplot(fig)

    if country_col:
        st.subheader("Top 10 pays")
        top = df[country_col].value_counts().head(10).reset_index()
        top.columns = ["Pays", "Nombre"]
        fig = px.bar(top, x="Pays", y="Nombre")
        st.plotly_chart(fig)

    st.subheader("Nuages de mots")
    text_pos = " ".join(df[df["label"] == 1][review_text_col].dropna())
    text_neg = " ".join(df[df["label"] == -1][review_text_col].dropna())

    col1, col2 = st.columns(2)
    if text_pos:
        wc = WordCloud(width=500, height=400, background_color="white").generate(text_pos)
        fig, ax = plt.subplots()
        ax.imshow(wc)
        ax.axis("off")
        ax.set_title("Positifs")
        col1.pyplot(fig)

    if text_neg:
        wc = WordCloud(width=500, height=400, background_color="white").generate(text_neg)
        fig, ax = plt.subplots()
        ax.imshow(wc)
        ax.axis("off")
        ax.set_title("N√©gatifs")
        col2.pyplot(fig)

elif page == "Classification des Sentiments":
    st.title("ü§ñ Classification des Sentiments")

    st.subheader("‚öôÔ∏è Param√®tres")

    test_size = st.slider(
        "Taille du jeu de test (%)",
        min_value=10,
        max_value=50,
        value=20,
        step=5
    ) / 100

    models = st.multiselect(
        "Choisir les mod√®les",
        ["Logistic Regression", "Naive Bayes"],
        default=["Logistic Regression", "Naive Bayes"]
    )

    if st.button("üöÄ Lancer la classification"):
        with st.spinner("Entra√Ænement et √©valuation des mod√®les..."):
            results = run_classification(
                df,
                review_text_col,
                "label",
                test_size,
                models
            )

        valid_results = {k: v for k, v in results.items() if "error" not in v}

        if not valid_results:
            st.error("‚ùå Aucun mod√®le n'a pu √™tre entra√Æn√©.")
            for model_name, result in results.items():
                if "error" in result:
                    st.error(f"{model_name} : {result['error']}")
            st.stop()

        # =========================
        # Infos globales Train/Test
        # =========================
        st.subheader("üìå R√©partition des donn√©es")

        example_model = next(iter(valid_results.values()))

        col1, col2, col3, col4 = st.columns(4)
        col1.metric("Train (%)", f"{example_model['train_percent']} %")
        col2.metric("Test (%)", f"{example_model['test_percent']} %")
        col3.metric("Train size", example_model["train_size"])
        col4.metric("Test size", example_model["test_size"])

        # =========================
        # Comparaison des mod√®les
        # =========================
        st.subheader("üèÅ Comparaison des mod√®les")

        best_model = max(valid_results.items(), key=lambda x: x[1]["accuracy"])
        st.success(
            f"üèÜ **Meilleur mod√®le : {best_model[0]}** "
            f"(Accuracy = {best_model[1]['accuracy']:.4f})"
        )

        fig = px.bar(
            x=list(valid_results.keys()),
            y=[v["accuracy"] for v in valid_results.values()],
            text=[f"{v['accuracy']:.3f}" for v in valid_results.values()],
            labels={"x": "Mod√®le", "y": "Accuracy"},
            title="Comparaison des accuracies"
        )
        st.plotly_chart(fig, use_container_width=True)

        # =========================
        # D√©tails par mod√®le
        # =========================
        st.subheader("üìã D√©tails des mod√®les")

        for model_name, model in valid_results.items():
            with st.expander(f"üìä {model_name}"):

                col1, col2 = st.columns(2)
                col1.metric("Accuracy", f"{model['accuracy']:.4f}")
                col2.metric("Temps d'entra√Ænement (s)", model["training_time"])

                st.markdown("**üìë Rapport de classification**")
                st.dataframe(pd.DataFrame(model["report"]).transpose())

                st.markdown("**üîç Matrice de confusion**")
                fig, ax = plt.subplots(figsize=(6, 5))
                sns.heatmap(
                    model["confusion_matrix"],
                    annot=True,
                    fmt="d",
                    cmap="Blues",
                    ax=ax,
                    xticklabels=["N√©gatif", "Neutre", "Positif"],
                    yticklabels=["N√©gatif", "Neutre", "Positif"]
                )
                ax.set_xlabel("Pr√©diction")
                ax.set_ylabel("R√©el")
                ax.set_title(f"Matrice de confusion ‚Äì {model_name}")
                st.pyplot(fig)

      
        st.success(
            "üíæ Le meilleur mod√®le a √©t√© sauvegard√© avec le vectoriseur TF-IDF "
            "dans le fichier **sentiment_model.pkl**"
        )

elif page == "Dataset Nettoy√©":
    st.title("üíæ Dataset Nettoy√©")

    cols = [review_text_col, "numeric_rating", "label"]
    if review_title_col:
        cols.insert(1, review_title_col)
    if country_col:
        cols.append(country_col)

    st.dataframe(df[cols].reset_index(drop=True))

    st.download_button(
        "üì• T√©l√©charger le dataset",
        data=df[cols].to_csv(index=False).encode("utf-8"),
        file_name="amazon_reviews_cleaned.csv",
        mime="text/csv"
    )

elif page == "Analyse via Transformers":
    st.title("ü§ñ Analyse de Sentiments via Transformers")
    
    st.markdown("""
    Cette page utilise des mod√®les **Transformers pr√©-entra√Æn√©s** de Hugging Face 
    pour l'analyse de sentiments. Ces mod√®les utilisent des architectures avanc√©es 
    (BERT, CamemBERT, XLM-RoBERTa) et offrent g√©n√©ralement de meilleures performances 
    que les m√©thodes TF-IDF classiques.
    """)

    if review_text_col is None:
        st.error("‚ùå Aucune colonne de texte d√©tect√©e dans le dataset.")
        st.stop()

    st.subheader("‚öôÔ∏è Configuration du mod√®le")
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        model_name = st.selectbox(
            "S√©lectionnez le mod√®le Hugging Face",
            options=[
                "cmarkea/distilcamembert-base-sentiment",
                "cardiffnlp/twitter-xlm-roberta-base-sentiment",
                "nlptown/bert-base-multilingual-uncased-sentiment",
            ],
            index=0,
            help="CamemBERT pour fran√ßais, XLM-RoBERTa pour multilingue"
        )
    
    with col2:
        st.info(f"""
        **Mod√®le actuel:**  
        `{model_name.split('/')[-1][:25]}...`
        """)

    # =========================
    # Pr√©diction texte unique
    # =========================
    st.subheader("üìù Test sur un texte unique")
    
    with st.expander("‚úçÔ∏è Entrez votre texte", expanded=True):
        user_text = st.text_area(
            "Texte √† analyser",
            value="Ce produit est absolument fantastique ! Je le recommande vivement.",
            height=120,
            help="Entrez n'importe quel avis ou commentaire"
        )
        
        col1, col2 = st.columns([1, 3])
        with col1:
            predict_btn = st.button("üîÆ Analyser", type="primary", use_container_width=True)
        
        if predict_btn and user_text.strip():
            try:
                with st.spinner("üîÑ Analyse en cours..."):
                    res = hf_predict_text(user_text, model_name)
                
                if "error" in res:
                    st.error(f"‚ùå {res['error']}")
                else:
                    st.success("‚úÖ Analyse termin√©e !")
                    
                    col1, col2, col3 = st.columns(3)
                    
                    sentiment_emoji = {1: "üòä", 0: "üòê", -1: "üòû"}
                    sentiment_name = {1: "Positif", 0: "Neutre", -1: "N√©gatif"}
                    sentiment_color = {1: "normal", 0: "off", -1: "inverse"}
                    
                    mapped = res["mapped"]
                    
                    col1.metric(
                        "Sentiment",
                        f"{sentiment_emoji[mapped]} {sentiment_name[mapped]}",
                        delta=None
                    )
                    col2.metric(
                        "Confiance",
                        f"{res['score']:.1%}",
                        delta=None
                    )
                    col3.metric(
                        "Label brut",
                        res["label"],
                        delta=None
                    )
            except Exception as e:
                st.error(f"‚ùå Erreur lors de l'analyse: {str(e)}")
                st.exception(e)

    # =========================
    # √âvaluation sur dataset
    # =========================
    st.subheader("üìä √âvaluation sur le dataset")
    
    st.markdown("""
    Testez le mod√®le sur un √©chantillon de votre dataset pour √©valuer ses performances 
    et comparer avec les mod√®les ML classiques.
    """)
    
    col1, col2 = st.columns([2, 1])
    
    with col1:
        sample_size = st.slider(
            "Taille de l'√©chantillon",
            min_value=50,
            max_value=min(2000, len(df)),
            value=min(500, len(df)),
            step=50,
            help="Plus l'√©chantillon est grand, plus l'√©valuation est pr√©cise (mais plus lente)"
        )
    
    with col2:
        st.metric("Dataset total", f"{len(df):,} avis")
    
    eval_btn = st.button("üöÄ Lancer l'√©valuation", type="primary", use_container_width=True)
    
    # Initialiser les cl√©s du session_state si elles n'existent pas
    if "eval_results" not in st.session_state:
        st.session_state.eval_results = None
    
    # Ex√©cuter l'√©valuation et stocker les r√©sultats dans session_state
    if eval_btn:
        try:
            with st.spinner(f"üîÑ Inf√©rence sur {sample_size} avis en cours..."):
                # √âchantillonnage stratifi√© si possible
                if "label" in df.columns and df["label"].nunique() > 1:
                    df_sample = df.groupby("label", group_keys=False).apply(
                        lambda x: x.sample(min(len(x), sample_size // df["label"].nunique()), random_state=42)
                    ).sample(frac=1, random_state=42).head(sample_size)
                else:
                    df_sample = df.sample(n=sample_size, random_state=42)
                
                # R√©initialiser l'index de df_sample pour garantir la coh√©rence
                df_sample = df_sample.reset_index(drop=True)
                
                # Ajouter un identifiant unique pour tra√ßabilit√©
                df_sample["eval_id"] = range(len(df_sample))
                
                texts = df_sample[review_text_col].astype(str).tolist()
                results = hf_predict_batch(texts, model_name)
                
                if not results:
                    st.error("‚ùå Aucun r√©sultat. V√©rifiez le mod√®le et la connexion internet.")
                    st.stop()
                
                df_pred = pd.DataFrame(results).reset_index(drop=True)
                df_pred["eval_id"] = range(len(df_pred))
                
                # Cr√©er le DataFrame merged pour garantir la coh√©rence absolue
                # Les deux DataFrames ont maintenant des index align√©s (0, 1, 2, ...)
                merged = pd.concat([
                    df_sample[["eval_id", review_text_col, "label"]],
                    df_pred[["label", "score", "mapped"]].add_prefix("hf_")
                ], axis=1)
                
                # V√©rification de coh√©rence
                assert len(df_sample) == len(df_pred) == len(merged), "D√©salignement d√©tect√© !"
                
                # Stocker les r√©sultats dans session_state pour persistance
                st.session_state.eval_results = {
                    "df_sample": df_sample,
                    "df_pred": df_pred,
                    "sample_size": sample_size,
                    "model_name": model_name,
                    "review_text_col": review_text_col,
                    "merged": merged
                }
                
                st.success(f"‚úÖ √âvaluation termin√©e ! {len(merged)} pr√©dictions fig√©es dans l'√©tat de l'application.")
        except Exception as e:
            st.error(f"‚ùå Erreur lors de l'√©valuation : {str(e)}")
            st.session_state.eval_results = None
    
    # Afficher les r√©sultats uniquement si une √©valuation a √©t√© effectu√©e
    if st.session_state.eval_results is not None:
        # R√©cup√©rer les r√©sultats stock√©s
        df_sample = st.session_state.eval_results["df_sample"]
        df_pred = st.session_state.eval_results["df_pred"]
        sample_size = st.session_state.eval_results["sample_size"]
        stored_model = st.session_state.eval_results["model_name"]
        review_text_col = st.session_state.eval_results["review_text_col"]
        merged = st.session_state.eval_results["merged"]
        
        # Afficher un bandeau de confirmation de coh√©rence
        st.info(f"üîí **R√©sultats fig√©s** : {sample_size} pr√©dictions synchronis√©es entre affichage et export")
        
        # Avertir si le mod√®le a chang√© depuis l'√©valuation
        if stored_model != model_name:
            st.warning(f"‚ö†Ô∏è Les r√©sultats affich√©s proviennent du mod√®le **{stored_model}**. Relancez l'√©valuation pour utiliser **{model_name}**.")
        
        try:
            # =========================
            # Distribution des pr√©dictions
            # =========================
            st.markdown("---")
            st.subheader("üìà Distribution des pr√©dictions")
            
            dist = df_pred["mapped"].value_counts().sort_index()
            
            fig = px.bar(
                x=["N√©gatif (-1)", "Neutre (0)", "Positif (+1)"],
                y=[dist.get(-1, 0), dist.get(0, 0), dist.get(1, 0)],
                labels={"x": "Sentiment", "y": "Nombre d'avis"},
                title=f"R√©partition des sentiments pr√©dits (n={sample_size})",
                color=["N√©gatif (-1)", "Neutre (0)", "Positif (+1)"],
                color_discrete_map={
                    "N√©gatif (-1)": "#ef4444",
                    "Neutre (0)": "#f59e0b",
                    "Positif (+1)": "#10b981"
                }
            )
            fig.update_layout(showlegend=False)
            st.plotly_chart(fig, use_container_width=True)
            
            col1, col2, col3 = st.columns(3)
            col1.metric("üòû N√©gatifs", dist.get(-1, 0), f"{100*dist.get(-1, 0)/sample_size:.1f}%")
            col2.metric("üòê Neutres", dist.get(0, 0), f"{100*dist.get(0, 0)/sample_size:.1f}%")
            col3.metric("üòä Positifs", dist.get(1, 0), f"{100*dist.get(1, 0)/sample_size:.1f}%")
            
            # =========================
            # M√©triques de performance
            # =========================
            if "label" in df_sample.columns:
                st.markdown("---")
                st.subheader("üéØ Performance du mod√®le")
                
                y_true = df_sample["label"].tolist()
                y_pred = df_pred["mapped"].tolist()
                
                from sklearn.metrics import accuracy_score, classification_report, confusion_matrix
                
                acc = accuracy_score(y_true, y_pred)
                
                # M√©triques principales
                col1, col2, col3 = st.columns(3)
                col1.metric("üéØ Accuracy", f"{acc:.2%}")
                
                report = classification_report(
                    y_true, y_pred,
                    labels=[-1, 0, 1],
                    target_names=["N√©gatif", "Neutre", "Positif"],
                    zero_division=0,
                    output_dict=True
                )
                
                col2.metric("üìä F1-Score (macro)", f"{report['macro avg']['f1-score']:.2%}")
                col3.metric("üìä Precision (macro)", f"{report['macro avg']['precision']:.2%}")
                
                # Rapport d√©taill√©
                with st.expander("üìã Rapport de classification d√©taill√©"):
                    df_report = pd.DataFrame(report).transpose()
                    st.dataframe(
                        df_report.style.format("{:.3f}"),
                        use_container_width=True
                    )
                
                # Matrice de confusion
                st.markdown("**üîç Matrice de confusion**")
                
                cm = confusion_matrix(y_true, y_pred, labels=[-1, 0, 1])
                
                fig_cm, ax = plt.subplots(figsize=(8, 6))
                sns.heatmap(
                    cm,
                    annot=True,
                    fmt="d",
                    cmap="Blues",
                    ax=ax,
                    xticklabels=["N√©gatif", "Neutre", "Positif"],
                    yticklabels=["N√©gatif", "Neutre", "Positif"],
                    cbar_kws={"label": "Nombre d'avis"}
                )
                ax.set_xlabel("Pr√©diction", fontsize=12)
                ax.set_ylabel("R√©el", fontsize=12)
                ax.set_title(f"Matrice de confusion ‚Äì {model_name.split('/')[-1]}", fontsize=14, pad=20)
                st.pyplot(fig_cm)
                
                # Analyse des erreurs
                with st.expander("üîé Exemples d'erreurs de classification"):
                    df_errors = df_sample.copy()
                    df_errors["prediction"] = y_pred
                    df_errors["correct"] = df_errors["label"] == df_errors["prediction"]
                    df_errors_only = df_errors[~df_errors["correct"]].head(10)
                    
                    if len(df_errors_only) > 0:
                        for idx, row in df_errors_only.iterrows():
                            sentiment_name = {1: "Positif", 0: "Neutre", -1: "N√©gatif"}
                            st.markdown(f"""
                            **Texte:** {row[review_text_col][:200]}...  
                            **R√©el:** {sentiment_name[row['label']]} | **Pr√©dit:** {sentiment_name[row['prediction']]}
                            """)
                            st.markdown("---")
                    else:
                        st.success("‚úÖ Aucune erreur dans les 10 premiers exemples !")
        
        except Exception as e:
            st.error(f"‚ùå Erreur lors de l'affichage: {str(e)}")
            st.exception(e)